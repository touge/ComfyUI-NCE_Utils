import torch.cuda
import gc
import comfy.model_management

from ..libs.utils import AnyType, clear_memory
any = AnyType("*")

CATEGORY = "ğŸ NCE/Utils"

class ClearVRAM:
    """
    æ¸…ç†VRAMå’Œç³»ç»Ÿå†…å­˜çš„èŠ‚ç‚¹
    
    åŠŸèƒ½è¯´æ˜:
        - å¸è½½å·²åŠ è½½çš„æ¨¡å‹
        - æ¸…ç†GPUæ˜¾å­˜ç¼“å­˜
        - æ‰§è¡ŒPythonåƒåœ¾å›æ”¶
    """

    def __init__(self):
        self.NODE_NAME = 'ClearVRAM'

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "anything": (any, {}),
                "clear_cache": ("BOOLEAN", {"default": True}),
                "clear_models": ("BOOLEAN", {"default": True}),
            },
            "optional": {
            }
        }

    RETURN_TYPES = (any,)
    RETURN_NAMES = ("any",)
    FUNCTION = "clear_vram"
    CATEGORY = CATEGORY
    OUTPUT_NODE = True

    def clear_vram(self, anything, clear_cache, clear_models):
        """
        æ¸…ç†æ˜¾å­˜å’Œå†…å­˜
        
        Args:
            anything: ä»»æ„è¾“å…¥ï¼Œç”¨äºè¿æ¥å·¥ä½œæµ
            clear_cache: æ˜¯å¦æ¸…ç†GPUç¼“å­˜
            clear_models: æ˜¯å¦å¸è½½æ‰€æœ‰æ¨¡å‹
        """
        # å¸è½½æ‰€æœ‰æ¨¡å‹
        if clear_models:
            comfy.model_management.unload_all_models()
            comfy.model_management.cleanup_models()
        
        # æ¸…ç†GPUç¼“å­˜
        if clear_cache:
            clear_memory()
            comfy.model_management.soft_empty_cache()
        
        # æ‰§è¡Œåƒåœ¾å›æ”¶
        gc.collect()
        
        return (anything,)

